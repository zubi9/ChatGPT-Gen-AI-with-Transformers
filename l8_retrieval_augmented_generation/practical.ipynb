{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "- Install GPT4All to run locally LLMs: https://gpt4all.io/index.html\n",
    "\n",
    "- Within GPT4All, setup `Llama 3 instruct` and `SBERT` for your RAG application.\n",
    "\n",
    "- Set a folder as a database, and populate it with files from your choosing (.pdf or .txt.). \n",
    "\n",
    "- If you want to use the publication database of DIT, you can use papers published since 2022, as they are less likely to make the model rely on its learned vectors. (https://www.th-deg.de/publication-database)\n",
    "\n",
    "- Deactivate the database and compare the quality of the retrieved information.\n",
    "\n",
    "- Explain how Llama-3 is able to be run on your local machine.\n",
    "\n",
    "\n",
    "### Advanced:\n",
    "- Instead of using the GPT4All models, write Python code to retrieve the relevant context with SBERT, and use the DIT API for LLMs to send the context with a prompt and generate a text that answers the prompt with that relevant context. (DIT API: http://vnesim.th-deg.de:8080/). \n",
    "\n",
    "For example: you retrieve 3 paragraphs from your indexed local database, and then you send the model \"(insert the 3 paragraphs). Taking this text as context, (insert the question)\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
