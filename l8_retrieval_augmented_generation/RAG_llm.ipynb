{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "- Install GPT4All to run locally LLMs: https://gpt4all.io/index.html\n",
    "\n",
    "- Within GPT4All, setup `Llama 3 instruct` and `SBERT` for your RAG application.\n",
    "\n",
    "- Set a folder as a database, and populate it with files from your choosing (.pdf or .txt.). \n",
    "\n",
    "- If you want to use the publication database of DIT, you can use papers published since 2022, as they are less likely to make the model rely on its learned vectors. (https://www.th-deg.de/publication-database)\n",
    "\n",
    "- Deactivate the database and compare the quality of the retrieved information.\n",
    "\n",
    "- Explain how Llama-3 is able to be run on your local machine.\n",
    "\n",
    "\n",
    "### Advanced:\n",
    "- Instead of using the GPT4All models, write Python code to retrieve the relevant context with SBERT, and use the DIT API for LLMs to send the context with a prompt and generate a text that answers the prompt with that relevant context. (DIT API: http://vnesim.th-deg.de:8080/). \n",
    "\n",
    "For example: you retrieve 3 paragraphs from your indexed local database, and then you send the model \"(insert the 3 paragraphs). Taking this text as context, (insert the question)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import streamlit as st\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "AWS_REGION = 'us-east-1'\n",
    "## Initialize Bedrock Clients\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\",region_name = AWS_REGION)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock)\n",
    "\n",
    "## Data ingestion function\n",
    "def data_ingestion():\n",
    "    loader = PyPDFDirectoryLoader(\"data\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "## Vector Embedding and vector store creation\n",
    "def get_vector_store(docs):\n",
    "    if not os.path.exists(\"faiss_index\"):\n",
    "        os.makedirs(\"faiss_index\")\n",
    "    vectorstore_faiss = FAISS.from_documents(docs, bedrock_embeddings)\n",
    "    vectorstore_faiss.save_local(\"faiss_index\")\n",
    "\n",
    "## Function to get Llama3 LLM\n",
    "def get_llama3_llm():\n",
    "    llm = Bedrock(model_id=\"meta.llama3-8b-instruct-v1:0\", client=bedrock, model_kwargs={'max_gen_len': 512})\n",
    "    return llm\n",
    "\n",
    "## Prompt template\n",
    "prompt_template = \"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end but use at least summarize with 250 words with detailed explanations. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "## Function to get response from LLM\n",
    "def get_response_llm(llm, vectorstore_faiss, query):\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_faiss.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    answer = qa({\"query\": query})\n",
    "    return answer['result']\n",
    "\n",
    "## Streamlit main function\n",
    "def main():\n",
    "    st.set_page_config(\"Chat PDF\")\n",
    "    st.header(\"Chat with PDF using AWS Bedrock\")\n",
    "\n",
    "    user_question = st.text_input(\"Ask a Question from the PDF Files\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.title(\"Update Or Create Vector Store:\")\n",
    "        if st.button(\"Vectors Update\"):\n",
    "            with st.spinner(\"Processing....\"):\n",
    "                docs = data_ingestion()\n",
    "                get_vector_store(docs)\n",
    "                st.success(\"Done\")\n",
    "\n",
    "    if st.button(\"Llama Output\"):\n",
    "        with st.spinner(\"Processing.....\"):\n",
    "            if not os.path.exists(\"faiss_index/index.faiss\"):\n",
    "                st.error(\"FAISS index file not found. Please update vectors first.\")\n",
    "            else:\n",
    "                faiss_index = FAISS.load_local(\"faiss_index\", bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "                llm = get_llama3_llm()\n",
    "                st.write(get_response_llm(llm, faiss_index, user_question))\n",
    "                st.success(\"Done\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
