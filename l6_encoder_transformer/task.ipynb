{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers for Next-Activity Prediction\n",
    "\n",
    "In this repository, there exist three scripts. Fill out this notebook to explain the code within them.\n",
    "\n",
    "(concept taken from https://arxiv.org/abs/2104.00721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "(explanation for the layers, type of positional encoding... suggestions of improvements maybe...)\n",
    "\n",
    "- Optional task: the inclusion of the findings in paper about positional encoding layer when dealing with time-series data: https://link.springer.com/article/10.1007/s10618-023-00948-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 15:45:31.547283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-01 15:45:31.575858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-01 15:45:31.575893: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-01 15:45:31.595167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 15:45:32.775007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# the code that is talked about\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_a = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_b = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_a = layers.Dropout(rate)\n",
    "        self.dropout_b = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout_a(attn_output, training=training)\n",
    "        out_a = self.layernorm_a(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out_a)\n",
    "        ffn_output = self.dropout_b(ffn_output, training=training)\n",
    "        return self.layernorm_b(out_a + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def get_model(max_case_length, vocab_size, output_dim, \n",
    "    embed_dim = 36, num_heads = 4, ff_dim = 64):\n",
    "    inputs = layers.Input(shape=(max_case_length,))\n",
    "    x = TokenAndPositionEmbedding(max_case_length, vocab_size, embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x, training=True)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(output_dim, activation=\"linear\")(x)\n",
    "    transformer = tf.keras.Model(inputs=inputs, outputs=outputs,\n",
    "        name = \"my_transformer\")\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "(What is the dataset, what is the vocabulary size, why is it processed the way it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import utils\n",
    "\n",
    "class LogsDataLoader:\n",
    "    def __init__(self, name, dir_path = \"./datasets\"):\n",
    "        \"\"\"Provides support for reading and \n",
    "            pre-processing examples from processed logs.\n",
    "        Args:\n",
    "            name: str: name of the dataset as used during processing raw logs\n",
    "            dir_path: str: Path to dataset directory\n",
    "        \"\"\"\n",
    "        self._dir_path = f\"{dir_path}/{name}/processed\"\n",
    "\n",
    "    def prepare_data_next_activity(self, df, \n",
    "        x_word_dict, y_word_dict, \n",
    "        max_case_length, shuffle=True):\n",
    "        \n",
    "        x = df[\"prefix\"].values\n",
    "        y = df[\"next_act\"].values\n",
    "        if shuffle:\n",
    "            x, y = utils.shuffle(x, y)\n",
    "\n",
    "        token_x = list()\n",
    "        for _x in x:\n",
    "            token_x.append([x_word_dict[s] for s in _x.split()])\n",
    "        # token_x = np.array(token_x, dtype = np.float32)\n",
    "\n",
    "        token_y = list()\n",
    "        for _y in y:\n",
    "            token_y.append(y_word_dict[_y])\n",
    "        # token_y = np.array(token_y, dtype = np.float32)\n",
    "\n",
    "        token_x = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            token_x, maxlen=max_case_length)\n",
    "\n",
    "        token_x = np.array(token_x, dtype=np.float32)\n",
    "        token_y = np.array(token_y, dtype=np.float32)\n",
    "\n",
    "        return token_x, token_y\n",
    "\n",
    "    def get_max_case_length(self, train_x):\n",
    "        train_token_x = list()\n",
    "        for _x in train_x:\n",
    "            train_token_x.append(len(_x.split()))\n",
    "        return max(train_token_x)\n",
    "\n",
    "    def load_data(self, task):\n",
    "        train_df = pd.read_csv(f\"{self._dir_path}/{task}_train.csv\")\n",
    "        test_df = pd.read_csv(f\"{self._dir_path}/{task}_test.csv\")\n",
    "\n",
    "        with open(f\"{self._dir_path}/metadata.json\", \"r\") as json_file:\n",
    "            metadata = json.load(json_file)\n",
    "\n",
    "        x_word_dict = metadata[\"x_word_dict\"]\n",
    "        y_word_dict = metadata[\"y_word_dict\"]\n",
    "        max_case_length = self.get_max_case_length(train_df[\"prefix\"].values)\n",
    "        vocab_size = len(x_word_dict) \n",
    "        total_classes = len(y_word_dict)\n",
    "\n",
    "        return (train_df, test_df, \n",
    "            x_word_dict, y_word_dict, \n",
    "            max_case_length, vocab_size, \n",
    "            total_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Using The Model\n",
    "\n",
    "(model compilation, loss function, accuracy metrics, data loading...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1107/1107 - 10s - 9ms/step - loss: 0.7255 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 2/10\n",
      "1107/1107 - 5s - 5ms/step - loss: 0.5834 - sparse_categorical_accuracy: 0.8183\n",
      "Epoch 3/10\n",
      "1107/1107 - 6s - 6ms/step - loss: 0.5747 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 4/10\n",
      "1107/1107 - 5s - 4ms/step - loss: 0.5728 - sparse_categorical_accuracy: 0.8191\n",
      "Epoch 5/10\n",
      "1107/1107 - 5s - 4ms/step - loss: 0.5716 - sparse_categorical_accuracy: 0.8193\n",
      "Epoch 6/10\n",
      "1107/1107 - 5s - 5ms/step - loss: 0.5672 - sparse_categorical_accuracy: 0.8194\n",
      "Epoch 7/10\n",
      "1107/1107 - 5s - 5ms/step - loss: 0.5654 - sparse_categorical_accuracy: 0.8178\n",
      "Epoch 8/10\n",
      "1107/1107 - 6s - 5ms/step - loss: 0.5627 - sparse_categorical_accuracy: 0.8195\n",
      "Epoch 9/10\n",
      "1107/1107 - 5s - 5ms/step - loss: 0.5657 - sparse_categorical_accuracy: 0.8201\n",
      "Epoch 10/10\n",
      "1107/1107 - 5s - 4ms/step - loss: 0.5631 - sparse_categorical_accuracy: 0.8193\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Average accuracy across all prefixes: 0.5727138588945734\n",
      "Average f-score across all prefixes: 0.508262042926925\n",
      "Average precision across all prefixes: 0.491929038057887\n",
      "Average recall across all prefixes: 0.539845694194156\n"
     ]
    }
   ],
   "source": [
    "# the code that is talked about\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics \n",
    "\n",
    "import loader\n",
    "import architecture\n",
    "\n",
    "\n",
    "dataset=\"helpdesk\"\n",
    "model_dir=\"./models\"\n",
    "result_dir=\"./results\"\n",
    "task = \"next_activity\"\n",
    "\n",
    "epochs=10\n",
    "batch_size=12\n",
    "learning_rate=0.001\n",
    "gpu=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories to save the results and models\n",
    "    model_path = f\"{model_dir}/{dataset}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    model_path = f\"{model_path}/next_activity_ckpt.weights.h5\"\n",
    "\n",
    "    result_path = f\"{result_dir}/{dataset}\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    result_path = f\"{result_path}/results\"\n",
    "\n",
    "    # Load data\n",
    "    data_loader = loader.LogsDataLoader(name=dataset)\n",
    "\n",
    "    (train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "        vocab_size, num_output) = data_loader.load_data(task)\n",
    "    \n",
    "    # Prepare training examples for next activity prediction task\n",
    "    train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "        x_word_dict, y_word_dict, max_case_length)\n",
    "    \n",
    "    # Create and train a transformer model\n",
    "    transformer_model = architecture.get_model(\n",
    "        max_case_length=max_case_length, \n",
    "        vocab_size=vocab_size,\n",
    "        output_dim=num_output)\n",
    "\n",
    "    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"sparse_categorical_accuracy\",\n",
    "        mode=\"max\", save_best_only=True)\n",
    "\n",
    "\n",
    "    transformer_model.fit(train_token_x, train_token_y, \n",
    "        epochs=epochs, batch_size=batch_size, \n",
    "        shuffle=True, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # Evaluate over all the prefixes (k) and save the results\n",
    "    k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "    for i in range(max_case_length):\n",
    "        test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "        if len(test_data_subset) > 0:\n",
    "            test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "                x_word_dict, y_word_dict, max_case_length)   \n",
    "            y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "            accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "            precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                test_token_y, y_pred, average=\"weighted\")\n",
    "            k.append(i)\n",
    "            accuracies.append(accuracy)\n",
    "            fscores.append(fscore)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    k.append(i + 1)\n",
    "    accuracies.append(np.mean(accuracy))\n",
    "    fscores.append(np.mean(fscores))\n",
    "    precisions.append(np.mean(precisions))\n",
    "    recalls.append(np.mean(recalls))\n",
    "    print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "    print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "    print('Average precision across all prefixes:', np.mean(precisions))\n",
    "    print('Average recall across all prefixes:', np.mean(recalls))    \n",
    "    results_df = pd.DataFrame({\"k\":k, \"accuracy\":accuracies, \"fscore\": fscores, \n",
    "        \"precision\":precisions, \"recall\":recalls})\n",
    "    results_df.to_csv(result_path+\"_next_activity.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "830/830 - 8s - 9ms/step - loss: 0.7700 - sparse_categorical_accuracy: 0.7469\n",
      "Epoch 2/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5901 - sparse_categorical_accuracy: 0.8179\n",
      "Epoch 3/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5785 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 4/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5750 - sparse_categorical_accuracy: 0.8184\n",
      "Epoch 5/20\n",
      "830/830 - 5s - 6ms/step - loss: 0.5710 - sparse_categorical_accuracy: 0.8181\n",
      "Epoch 6/20\n",
      "830/830 - 6s - 7ms/step - loss: 0.5706 - sparse_categorical_accuracy: 0.8187\n",
      "Epoch 7/20\n",
      "830/830 - 10s - 12ms/step - loss: 0.5690 - sparse_categorical_accuracy: 0.8191\n",
      "Epoch 8/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5676 - sparse_categorical_accuracy: 0.8207\n",
      "Epoch 9/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5627 - sparse_categorical_accuracy: 0.8180\n",
      "Epoch 10/20\n",
      "830/830 - 8s - 10ms/step - loss: 0.5657 - sparse_categorical_accuracy: 0.8193\n",
      "Epoch 11/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5614 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 12/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5612 - sparse_categorical_accuracy: 0.8194\n",
      "Epoch 13/20\n",
      "830/830 - 5s - 5ms/step - loss: 0.5613 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 14/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5588 - sparse_categorical_accuracy: 0.8207\n",
      "Epoch 15/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5599 - sparse_categorical_accuracy: 0.8199\n",
      "Epoch 16/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5590 - sparse_categorical_accuracy: 0.8201\n",
      "Epoch 17/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5585 - sparse_categorical_accuracy: 0.8201\n",
      "Epoch 18/20\n",
      "830/830 - 5s - 6ms/step - loss: 0.5557 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 19/20\n",
      "830/830 - 5s - 6ms/step - loss: 0.5567 - sparse_categorical_accuracy: 0.8199\n",
      "Epoch 20/20\n",
      "830/830 - 4s - 5ms/step - loss: 0.5533 - sparse_categorical_accuracy: 0.8200\n",
      "Seed text contains no valid words from the vocabulary.\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Average accuracy across all prefixes: 0.5450373482548069\n",
      "Average f-score across all prefixes: 0.4837398978898834\n",
      "Average precision across all prefixes: 0.4810167807043203\n",
      "Average recall across all prefixes: 0.5100402211974843\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "import loader\n",
    "import architecture\n",
    "\n",
    "dataset = \"helpdesk\"\n",
    "model_dir = \"./models\"\n",
    "result_dir = \"./results\"\n",
    "task = \"next_activity\"\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "gpu = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "def generate_text(model, seed_text, x_word_dict, max_length):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained model to generate text.\n",
    "    - seed_text: Initial text to start the generation.\n",
    "    - x_word_dict: Dictionary mapping words to indices.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "\n",
    "    Returns:\n",
    "    - generated_text: Generated text as a string.\n",
    "    \"\"\"\n",
    "    reverse_word_dict = {v: k for k, v in x_word_dict.items()}  # Reverse the dictionary\n",
    "    seed_words = seed_text.split()\n",
    "\n",
    "    # Ensure seed_text has valid words\n",
    "    token_list = [x_word_dict[word] for word in seed_words if word in x_word_dict]\n",
    "    if not token_list:\n",
    "        raise ValueError(\"Seed text contains no valid words from the vocabulary.\")\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Pad the token list to the required max_case_length\n",
    "        padded_token_list = np.pad(token_list, (0, max_case_length - len(token_list)), mode='constant')\n",
    "        padded_token_list = np.array(padded_token_list).reshape(1, -1)\n",
    "\n",
    "        # Predict the next word\n",
    "        predicted = np.argmax(model.predict(padded_token_list), axis=-1)[0]\n",
    "\n",
    "        # Get the predicted word\n",
    "        output_word = reverse_word_dict.get(predicted, '')\n",
    "\n",
    "        # Append the word to the seed text\n",
    "        seed_words.append(output_word)\n",
    "        token_list.append(predicted)\n",
    "\n",
    "        # Stop if end of sentence token is generated\n",
    "        if output_word == 'endtoken':\n",
    "            break\n",
    "\n",
    "    return ' '.join(seed_words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories to save the results and models\n",
    "    model_path = f\"{model_dir}/{dataset}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    model_path = f\"{model_path}/next_activity_ckpt.weights.h5\"\n",
    "\n",
    "    result_path = f\"{result_dir}/{dataset}\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    result_path = f\"{result_path}/results\"\n",
    "\n",
    "    # Load data\n",
    "    data_loader = loader.LogsDataLoader(name=dataset)\n",
    "\n",
    "    (train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "        vocab_size, num_output) = data_loader.load_data(task)\n",
    "    \n",
    "    # Prepare training examples for next activity prediction task\n",
    "    train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "        x_word_dict, y_word_dict, max_case_length)\n",
    "    \n",
    "    # Create and train a transformer model\n",
    "    transformer_model = architecture.get_model(\n",
    "        max_case_length=max_case_length, \n",
    "        vocab_size=vocab_size,\n",
    "        output_dim=num_output)\n",
    "\n",
    "    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"sparse_categorical_accuracy\",\n",
    "        mode=\"max\", save_best_only=True)\n",
    "\n",
    "    transformer_model.fit(train_token_x, train_token_y, \n",
    "        epochs=epochs, batch_size=batch_size, \n",
    "        shuffle=True, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # Load the best model weights\n",
    "    transformer_model.load_weights(model_path)\n",
    "\n",
    "    # Generate text using the trained model\n",
    "    seed_text = \"Seriousness assigned\"\n",
    "    max_length = 100\n",
    "    try:\n",
    "        generated_text = generate_text(transformer_model, seed_text, x_word_dict, max_length)\n",
    "        print(\"Generated Text:\\n\", generated_text)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Evaluate over all the prefixes (k) and save the results\n",
    "    k, accuracies, fscores, precisions, recalls = [], [], [], [], []\n",
    "    for i in range(max_case_length):\n",
    "        test_data_subset = test_df[test_df[\"k\"] == i]\n",
    "        if len(test_data_subset) > 0:\n",
    "            test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "                x_word_dict, y_word_dict, max_case_length)\n",
    "            y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "            accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "            precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                test_token_y, y_pred, average=\"weighted\")\n",
    "            k.append(i)\n",
    "            accuracies.append(accuracy)\n",
    "            fscores.append(fscore)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    k.append(i + 1)\n",
    "    accuracies.append(np.mean(accuracy))\n",
    "    fscores.append(np.mean(fscores))\n",
    "    precisions.append(np.mean(precisions))\n",
    "    recalls.append(np.mean(recalls))\n",
    "    print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "    print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "    print('Average precision across all prefixes:', np.mean(precisions))\n",
    "    print('Average recall across all prefixes:', np.mean(recalls))\n",
    "    results_df = pd.DataFrame({\"k\": k, \"accuracy\": accuracies, \"fscore\": fscores, \n",
    "        \"precision\": precisions, \"recall\": recalls})\n",
    "    results_df.to_csv(result_path + \"_next_activity.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text contains no valid words from the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"resolve ticket\"\n",
    "max_length = 100\n",
    "try:\n",
    "    generated_text = generate_text(transformer_model, seed_text, x_word_dict, max_length)\n",
    "    print(\"Generated Text:\\n\", generated_text)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence contains no valid words from the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import loader\n",
    "import architecture\n",
    "\n",
    "# Settings\n",
    "dataset = \"helpdesk\"\n",
    "model_dir = \"./models\"\n",
    "task = \"next_activity\"\n",
    "gpu = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "def generate_next_activity(model, input_sequence, x_word_dict, max_case_length):\n",
    "    \"\"\"\n",
    "    Predict the next activity given an input sequence using the trained model.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained transformer model.\n",
    "    - input_sequence: List of activities (strings) representing the input sequence.\n",
    "    - x_word_dict: Dictionary mapping words to indices.\n",
    "    - max_case_length: Maximum length of the input sequence for padding.\n",
    "\n",
    "    Returns:\n",
    "    - next_activity: The predicted next activity as a string.\n",
    "    \"\"\"\n",
    "    reverse_word_dict = {v: k for k, v in x_word_dict.items()}  # Reverse the dictionary\n",
    "    token_list = [x_word_dict[word] for word in input_sequence if word in x_word_dict]\n",
    "\n",
    "    if not token_list:\n",
    "        raise ValueError(\"Input sequence contains no valid words from the vocabulary.\")\n",
    "\n",
    "    # Pad the token list to the required max_case_length\n",
    "    padded_token_list = np.pad(token_list, (0, max_case_length - len(token_list)), mode='constant')\n",
    "    padded_token_list = np.array(padded_token_list).reshape(1, -1)\n",
    "\n",
    "    # Predict the next word\n",
    "    predicted = np.argmax(model.predict(padded_token_list), axis=-1)[0]\n",
    "\n",
    "    # Get the predicted word\n",
    "    next_activity = reverse_word_dict.get(predicted, '')\n",
    "\n",
    "    return next_activity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data_loader = loader.LogsDataLoader(name=dataset)\n",
    "    (train_df, test_df, x_word_dict, y_word_dict, max_case_length, vocab_size, num_output) = data_loader.load_data(task)\n",
    "\n",
    "    # Create and load the transformer model\n",
    "    transformer_model = architecture.get_model(\n",
    "        max_case_length=max_case_length, \n",
    "        vocab_size=vocab_size,\n",
    "        output_dim=num_output)\n",
    "\n",
    "    model_path = f\"{model_dir}/{dataset}/next_activity_ckpt.weights.h5\"\n",
    "    transformer_model.load_weights(model_path)\n",
    "\n",
    "    # Example input sequence for inference\n",
    "    input_sequence = [\"Assign seriousness\", \"Value 1\", \"Take in charge ticket\"]\n",
    "\n",
    "\n",
    "    # Generate next activity\n",
    "    try:\n",
    "        next_activity = generate_next_activity(transformer_model, input_sequence, x_word_dict, max_case_length)\n",
    "        print(\"Next Activity:\", next_activity)\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
