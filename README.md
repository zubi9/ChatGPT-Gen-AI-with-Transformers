# ChatGPT: Gen AI with Transformers

This repository contains a series of practical exercises performed in Jupyter Notebooks, organized into directories. Each directory focuses on a specific topic related to generative AI and transformers. Below is an overview of the contents and activities within each directory.

## Directory Structure and Activities

### 1. `l1_markov_language_model`
- **Activities**:
  - Implementation of a Markov language model.
  - Understanding and building n-gram models.
  - Generating text using Markov chains.

### 2. `l2_embeddings_and_word2vec`
- **Activities**:
  - Introduction to word embeddings.
  - Implementation of Word2Vec using skip-gram and CBOW models.
  - Visualization of word embeddings.
  - Analysis of semantic similarity using embeddings.

### 3. `l3_rnn_language_model`
- **Activities**:
  - Understanding Recurrent Neural Networks (RNNs).
  - Building and training an RNN language model.
  - Generating text using RNNs.
  - Exploring the limitations of RNNs.

### 4. `l4_bidirectional_lstm_model`
- **Activities**:
  - Introduction to Long Short-Term Memory (LSTM) networks.
  - Implementation of bidirectional LSTMs.
  - Training and evaluating LSTM models for language tasks.
  - Comparison of bidirectional LSTMs with standard RNNs.

### 5. `l5_self-attention`
- **Activities**:
  - Understanding the concept of self-attention.
  - Implementing self-attention mechanisms.
  - Exploring the role of self-attention in sequence modeling.
  - Visualization of attention scores.

### 6. `l6_encoder_transformer`
- **Activities**:
  - Introduction to the Transformer architecture.
  - Building and training an encoder-only transformer model.
  - Applications of encoder transformers in text classification and encoding tasks.
  - Analysis of transformer performance.

### 7. `l7_masked_language_model`
- **Activities**:
  - Understanding masked language modeling (MLM).
  - Implementation of MLM using transformers.
  - Training and fine-tuning models for masked language tasks.
  - Evaluation of MLM performance on text prediction.

### 8. `l8_retrieval_augmented_generation`
- **Activities**:
  - Introduction to retrieval-augmented generation (RAG) models.
  - Combining retrieval mechanisms with generative models.
  - Implementation of RAG for enhanced text generation.
  - Practical applications and evaluation of RAG models.

### 9. `l9_transformers_beyond_just_natural_language`
- **Activities**:
  - Exploring the applications of transformers beyond natural language processing.
  - Implementing transformers for tasks in computer vision and other domains.
  - Understanding the versatility and adaptability of transformer models.
  - Case studies and practical examples of transformers in various fields.

Each directory contains Jupyter Notebooks with detailed explanations, code implementations, and practical exercises to enhance your understanding of generative AI and transformer models.

## Contributing
We welcome contributions from the community! If you have suggestions for improvements, new practical exercises, or additional topics, please feel free to submit a pull request. Ensure your contributions align with the overall structure and goals of this repository.

## Open to Use
This repository is open for use by anyone interested in learning about generative AI and transformers. You are free to use, modify, and distribute the contents as per the terms of the MIT License. We hope this resource proves valuable in your learning journey and helps you build a solid foundation in AI with transformers.

Happy learning!

